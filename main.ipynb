{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"notebook-buttons\" style=\"display:flex; padding-top: 5rem;padding-bottom: 2.5rem;line-height: 2.15;\">\n",
    "    <a href=\"https://colab.research.google.com/github/magdasalatka/fantastic-features/blob/main/main.ipynb\">\n",
    "        <div id=\"colab-link\" style=\"display: flex;padding-right: 3.5rem;padding-bottom: 0.625rem;border-bottom: 1px solid #ececed; align-items: center;\">\n",
    "            <img class=\"call-to-action-img\" src=\"img/colab.svg\" width=\"30\" height=\"30\" style=\"margin-right: 10px;margin-top: auto;margin-bottom: auto;\">\n",
    "            <div class=\"call-to-action-txt\">Run in Google Colab</div>\n",
    "        </div>\n",
    "    </a>\n",
    "    <a href=\"https://raw.githubusercontent.com/magdasalatka/fantastic-features/main/main.ipynb\" download>\n",
    "        <div id=\"download-link\" style=\"display: flex;padding-right: 3.5rem;padding-bottom: 0.625rem;border-bottom: 1px solid #ececed; height: auto;align-items: center;\">\n",
    "            <img class=\"call-to-action-img\" src=\"img/download.svg\" width=\"22\" height=\"30\" style=\"margin-right: 10px;margin-top: auto;margin-bottom: auto;\">\n",
    "            <div class=\"call-to-action-txt\">Download Notebook</div>\n",
    "        </div>\n",
    "    </a>\n",
    "    <a href=\"https://github.com/magdasalatka/fantastic-features/blob/main/main.ipynb\">\n",
    "        <div id=\"github-link\" style=\"display: flex;padding-right: 3.5rem;padding-bottom: 0.625rem;border-bottom: 1px solid #ececed; height: auto;align-items: center;\">\n",
    "            <img class=\"call-to-action-img\" src=\"img/github.svg\" width=\"25\" height=\"30\" style=\"margin-right: 10px;margin-top: auto;margin-bottom: auto;\">\n",
    "            <div class=\"call-to-action-txt\">View on GitHub</div>\n",
    "        </div>\n",
    "    </a>\n",
    "</div>\n",
    "\n",
    "# Back to the Feature\n",
    "### boost you model with statistical feature engineering\n",
    "by [Teresa Kubacka](http://teresa-kubacka.com/), [Magdalena Sur√≥wka](https://datali.ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.graphics as sg\n",
    "import math\n",
    "\n",
    "from dataset_noise_generator import *\n",
    "from plotting import plot_x_vs_y, plot_x_over_time, plot_fitted_vs_residuals, calculate_variances, plot_residuals_distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model diagnostics\n",
    "\n",
    "Regression has some assumptions about its errors, *e<sub>i</sub>*. \n",
    "These assumptions are:\n",
    "\n",
    " * E[*e<sub>i</sub>*] = 0\n",
    " * Var(*e<sub>i</sub>*) = &sigma;<sup>2</sup>\n",
    " * *e<sub>i</sub>* ~ *N*(0, &sigma;<sup>2</sup>)\n",
    " * Cov(*e<sub>i</sub>*, *e<sub>j</sub>*) = 0\n",
    "\n",
    " Additioanlly, we also assume:\n",
    " * no multicollinearity in regressors\n",
    "\n",
    "The first three conditions are necessary to perform a least square estimation and to have valid fitted values.  \n",
    "The last condition is only required for any hypothesis tests, confidence intervals and prediction intervals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, most of our assumptions are about errors. However, errors cannot be observed in practice. Instead, we well be working with residuals, which are only estimates of the errors. They have however, an estimation-related issue.   \n",
    "In regression, the variances of the residuals at different input variable values may differ. This can happen even if the variances of the errors at these different input variable values are equal.   \n",
    "\n",
    "To improve the results, we can standardize or [studentize](https://en.wikipedia.org/wiki/Studentized_residual) the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Sample model\n",
    "Let's fit a sample model, and take a look at its fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/interesting_data.csv\", index_col=0)\n",
    "y = df.y.to_numpy()\n",
    "X = df.loc[:, df.columns != 'y'].to_numpy()\n",
    "\n",
    "threshold = int(len(X)*0.8)\n",
    "X_train, X_test = X[:threshold], X[threshold:]\n",
    "y_train, y_test = y[:threshold], y[threshold:]\n",
    "\n",
    "predictors = sm.add_constant(X_train)\n",
    "model = sm.OLS(y_train, predictors).fit()\n",
    "fitted = model.predict(predictors)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Assumption 1\n",
    "E[*e<sub>i</sub>*] = 0\n",
    "\n",
    "In other words, on average the errrors should be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_raw = fitted - y_train\n",
    "residuals = residuals_raw/np.std(residuals_raw)\n",
    "print(\"Expected error estimate: {}\".format(sum(residuals)/len(residuals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x_vs_y(fitted, residuals, \"response\", \"residuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x_vs_y(fitted, residuals, \"Fitted\", \"Residuals\", add_zero_line=True, add_lowess=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failing assumption implies systematic error. This means:\n",
    "* the relationship between response and regressors may be nonlinear\n",
    "* some important regressors may be missing\n",
    "* some important interactions may be missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Assumption 2\n",
    "\n",
    "Var(*e<sub>i</sub>*) = &sigma;<sup>2</sup>\n",
    "\n",
    "In other words: variance of residuals should be constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Fitted vs residuals: heteroskedasticity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extend our plot_fitted_vs_residuals function\n",
    "def plot_fitted_vs_residuals(fitted: np.ndarray, residuals: np.ndarray, \n",
    "                             mean: bool=True, \n",
    "                             width = 100, \n",
    "                          bin_type = None) -> None:\n",
    "\n",
    "    ax = plot_x_vs_y(fitted, residuals, \"Fitted\", \"Residuals\")\n",
    "    \n",
    "    if mean:\n",
    "        ax.hlines(0, xmin=min(fitted), xmax=max(fitted), colors=\"orange\")\n",
    "    if bin_type == \"window\":\n",
    "        ax.vlines(np.arange(min(fitted), max(fitted)+width, width), \n",
    "                  ymin=min(residuals), ymax=max(residuals), colors=\"red\")\n",
    "    elif bin_type == \"points\": \n",
    "        width = round(width)\n",
    "        ax.vlines(np.sort(fitted)[::width], \n",
    "                      ymin=min(residuals), ymax=max(residuals), colors=\"red\")\n",
    "    plt.title('Fitted vs Residuals')\n",
    "\n",
    "plot_fitted_vs_residuals(fitted, residuals, mean=False, width=100, bin_type='window')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Residuals: heteroskedasticity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try different widths\n",
    "bins, variances, counts = calculate_variances(fitted, residuals, 100, bin_type=\"points\")\n",
    "\n",
    "plt.bar(bins, variances, width=np.diff(bins).min()*0.8, align='edge')\n",
    "plt.title('Variances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failing assumption implies heteroskedasticity. This means:\n",
    "* the error estimates are not valid =>\n",
    "* the confidence intervals are not valid =>\n",
    "* p-values are not valid\n",
    "* coefficients **are valid**  \n",
    "\n",
    "In short: your expected value remains unchanged. But you have no viable insights on model unsertanity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Assumption 3\n",
    "*e<sub>i</sub>* ~ *N*(0, &sigma;<sup>2</sup>)\n",
    "\n",
    "In other words: residuals are normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Empirical distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals_distribution(residuals, bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 QQ-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.qqplot(residuals/np.std(residuals), line='45')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failing assumption implies systematic deviation. This means:\n",
    "* model is failing to capture certain range of values\n",
    "* model structure is not correct\n",
    "\n",
    "In practice:\n",
    "* few data points that are slightly \"off the line\" near the ends common, and usually tolerable\n",
    "* skewed residuals need correction\n",
    "* long-tailed, but symmetrical residuals are can be tolerable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Assumption 4\n",
    "Cov(*e<sub>i</sub>*, *e<sub>j</sub>*) = 0\n",
    "\n",
    "In other words: errors are not correlated **WITH EACH OTHER**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Residuals over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x_over_time(residuals, \"Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 ACF plot\n",
    "Check residuals vs lagged residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sg.tsaplots.plot_acf(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failing assumption means:\n",
    "* estimates are unbiased => expected value for coefficients and predictions is ok\n",
    "* the estimate is not efficient => there are better regression models \n",
    "* standard errors are biased => confidence intervals, test statistics, and p-values are flawed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Assumption 5 (Optional)\n",
    "No multicollinearlty\n",
    "\n",
    "Regression does not have a unique solution if regressors are exactly linearly dependent. Often, we will find not perfect, but a strong correlation between variables. Multicollinearity means that there is such strong, yet not perfect, relation between the columns of X.\n",
    "\n",
    "Under multicollinity, unique solution exists. However, it performs poorly in practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1 Correlation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlations(X: np.ndarray) -> None:\n",
    "    df = pd.DataFrame(X)\n",
    "\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    plt.matshow(df.corr(), fignum=f.number)\n",
    "    plt.xticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14, rotation=45)\n",
    "    plt.yticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14)\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "    plt.title('Correlation Matrix', fontsize=16);\n",
    "\n",
    "plot_correlations(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearlity means:\n",
    "* estimated coefficients have large standard errors\n",
    "* the coefficients are imprecise\n",
    "* it happens that none of the regressors is significant!\n",
    "* small change in data can result in big change in results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Model diagnostics\n",
    "@Teresa: Anything here? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: load predefined dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "print(\"X shape: {}\".format(X.shape))\n",
    "print(\"y shape: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: work with a synthetic dataset: \n",
    "- sklearn's `make_regression`\n",
    "- modified version of `make_regression` that takes coefficients as an input \n",
    "- your own generated data (e.g. containing coupled variables, polynomial/non-linear relations etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_indep = 3\n",
    "n_sample = 1000\n",
    "\n",
    "X_base, y_base, coeffs = make_regression_custom(n_samples=n_sample, n_features=num_indep, n_informative=num_indep,\n",
    "                       tail_strength=0, bias=0, n_targets=1, noise=0, \n",
    "                           shuffle=False, coef=True, random_state=2, custom_coef=[30,15,20])\n",
    "# noise = gen_noise(X_base, 0.1, 'sin', n_osc=3)+gen_noise(X_base, 0.1, 'normal')\n",
    "# X = X_base + noise\n",
    "X = X_base\n",
    "\n",
    "# noise_y = gen_noise(y_base, 15, 'normal')*gen_noise(y_base, 0.1, 'linear_inc')\n",
    "# y = y_base + noise_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yn = gen_noise(y_base,30,noise_type=\"poisson\")*gen_noise(y_base,1,noise_type=\"normal\")\n",
    "\n",
    "y = y_base+yn\n",
    "\n",
    "plt.plot(y_base,yn,marker='.',lw=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yn = gen_noise(y_base,10,noise_type=\"linear_inc_index\")*gen_noise(y_base,1,noise_type=\"normal\")\n",
    "\n",
    "y = y_base+yn\n",
    "\n",
    "plt.plot(y_base,yn,marker='.',lw=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yn = gen_noise(y_base,1,noise_type=\"normal_prop\")\n",
    "\n",
    "y = y_base+yn\n",
    "\n",
    "plt.plot(y_base,yn,marker='.',lw=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yn = gen_noise(y_base,1,noise_type=\"linear_inc_index\", func=lambda x: np.log(np.abs(x)))*gen_noise(y_base,1,noise_type=\"normal\")\n",
    "# *gen_noise(y_base,1,noise_type=\"normal\")\n",
    "\n",
    "y = y_base+yn\n",
    "\n",
    "plt.plot(y_base,yn,marker='.',lw=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isneginf(yn).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yn = gen_noise(y_base,1,noise_type=\"linear_inc_index\", func=lambda x: 1/np.sqrt(x))*gen_noise(y_base,1,noise_type=\"normal\")\n",
    "# *gen_noise(y_base,1,noise_type=\"normal\")\n",
    "\n",
    "y = y_base+yn\n",
    "\n",
    "plt.plot(y_base,yn,marker='.',lw=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_base.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = int(len(X)*0.8)\n",
    "X_train, X_test = X[:threshold], X[threshold:]\n",
    "y_train, y_test = y[:threshold], y[threshold:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fit regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Inspect data\n",
    "How does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_x_vs_y(x: np.ndarray, y: np.ndarray, x_name: str = 'x_name', y_name: str = 'y_name', \n",
    "                         add_zero_line: bool = False, add_lowess: bool = False, \n",
    "                         model = None, hexbin: bool = False, \n",
    "                         figsize = None, ax = None, lowess_param = 0.2, plot_args = {}) -> None:\n",
    "    if ax is None: \n",
    "        _, axs = plt.subplots(1, 1, figsize=figsize, constrained_layout=True)\n",
    "        this_ax = axs\n",
    "    else:\n",
    "        this_ax = ax\n",
    "    \n",
    "    if hexbin: \n",
    "        this_ax.hexbin(x, y, cmap=\"Greys\", gridsize=40, **plot_args)\n",
    "    else: \n",
    "        scatter_defaults = {'alpha': 0.25, 's': 20, 'edgecolors':None}\n",
    "        for k,v in scatter_defaults.items():\n",
    "            if k not in plot_args.keys():\n",
    "                plot_args[k] = v        \n",
    "        this_ax.scatter(x, y, lw=0, **plot_args)\n",
    "\n",
    "    xlabel = x_name\n",
    "    this_ax.set_xlabel(xlabel)\n",
    "    this_ax.set_ylabel(y_name)\n",
    "    this_ax.grid(True)\n",
    "    if add_zero_line:\n",
    "        this_ax.hlines(0, xmin=min(x), xmax=max(x), colors=\"magenta\")\n",
    "    if add_lowess: \n",
    "        sorted_xy = np.array(sorted(zip(x,y))).T\n",
    "        sorted_x = sorted_xy[0,:]\n",
    "        sorted_y = sorted_xy[1,:]\n",
    "        smoothed = sm.nonparametric.lowess(exog=sorted_x, endog=sorted_y, frac=lowess_param)\n",
    "#         print(smoothed.shape)\n",
    "        this_ax.plot(smoothed[:,0],smoothed[:,1],lw=1,color='k')\n",
    "\n",
    "\n",
    "    return this_ax\n",
    "\n",
    "plot_x_vs_y(X_train[:,0], y_train, \"Independent variable\", \"Dependent variable\", \n",
    "                     add_zero_line=False, add_lowess=True,hexbin=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x_vs_y(X_train[:,0], y_train, \"Independent variable\", \"Dependent variable\", \n",
    "                     add_zero_line=True, hexbin=False, plot_args={'s':150, 'alpha':0.1});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_x_vs_y(X: np.ndarray, y: np.ndarray, x_name: str, y_name: str, \n",
    "                         add_regression_line: bool = False, add_zero_line: bool =False, \n",
    "                         model = None, hexbin: bool = False, n_col:int = 2,\n",
    "                         figsize = None, plot_args = {}) -> None:\n",
    "\n",
    "    if add_regression_line and (model is None):\n",
    "        predictors = sm.add_constant(X)\n",
    "        model = sm.OLS(y_train, predictors).fit()\n",
    "\n",
    "    if len(X.shape)>1:\n",
    "        n_vars = X.shape[1]\n",
    "    else: \n",
    "        n_vars = 1\n",
    "        X = X.reshape((len(X),1)) # otherwise X.T will enumerate the list element-wise\n",
    "    \n",
    "    n_row = math.ceil(n_vars/n_col)\n",
    "    if figsize is None: \n",
    "        subplot_size = (3,3)\n",
    "        figsize = (n_col*subplot_size[0], n_row*subplot_size[1])\n",
    "    _, axs = plt.subplots(n_row, n_col, figsize=figsize, constrained_layout=True)\n",
    "\n",
    "    for i, x in enumerate(X.T):\n",
    "        row = i//n_col\n",
    "        col = i%n_col\n",
    "        \n",
    "        if (n_vars == 1) and (n_col == 1) : \n",
    "            this_ax = axs\n",
    "        elif n_row == 1: \n",
    "            this_ax = axs[col]\n",
    "        else: \n",
    "            this_ax = axs[row,col]\n",
    "\n",
    "        if n_vars > 1: \n",
    "            xlabel = \"{} {}\".format(x_name, i)\n",
    "        else: \n",
    "            xlabel = x_name\n",
    "            \n",
    "        plot_x_vs_y(x=x, y=y, x_name=xlabel, y_name=y_name, \n",
    "                         add_zero_line= add_zero_line, \n",
    "                         model = model, hexbin = hexbin, \n",
    "                         figsize = figsize, ax = this_ax, plot_args=plot_args)     \n",
    "\n",
    "        if add_regression_line and (model is not None):\n",
    "            x_vals = np.array(this_ax.get_xlim())\n",
    "            y_vals = model.params[0] + model.params[i+1] * x_vals\n",
    "            this_ax.plot(x_vals, y_vals, '--', color=\"orange\")\n",
    "        \n",
    "    return axs\n",
    "\n",
    "plot_multiple_x_vs_y(X_train, y_train, \"Independent variable\", \"Dependent variable\", \n",
    "                     add_regression_line=True, hexbin=False, n_col=3, plot_args={'s':20});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn version => No summary table\n",
    "#lm = linear_model.LinearRegression()\n",
    "#model = lm.fit(X_train, y_train)\n",
    "#fitted = model.predict(X_train)\n",
    "\n",
    "predictors = sm.add_constant(X_train)\n",
    "model = sm.OLS(y_train, predictors).fit()\n",
    "fitted = model.predict(predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Analysing model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Statsmodels output table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with our coeffs: \n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the table to pandas dataframe using .summary2(); _.tables is a tuple of 3 sub-tables, each of them is a df \n",
    "model.summary2().tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .summary2() not always stable, a workaround is below, but then you lose some digits after comma:  \n",
    "pd.read_html(model.summary().tables[1].as_html(), header=0, index_col=0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Actual vs predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_ax = plot_x_vs_y(fitted, y_train, \"Predicted\", \"Observed\", figsize=(6,6))\n",
    "tmp_ax.plot([-150,150],[-150,150],lw=1,color='orange',zorder=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2:  Transformations\n",
    "\n",
    "## Transformations\n",
    "\n",
    "The idea behing data trasformations is to achieve a mean function that is linear in the transformed scale.\n",
    "\n",
    "The most commonly used methods to transform variables are:\n",
    "* Logarithmic transformation - np.log(X)\n",
    "* Reciprocal transformation - 1 / X\n",
    "* Square root transformation - X**(1/2)\n",
    "* Exponential transformation (more general, you can use any exponent)\n",
    "* Box-Cox transformation\n",
    "\n",
    "\n",
    "## Exercise\n",
    "\n",
    "In this exercise, we will practice variables transformation. \n",
    "\n",
    "First, load datasets from `data/ex1`.  \n",
    "Then, for each dataset:  \n",
    "* fit linear model: y = f(x) + e  \n",
    "* calculate residuals  \n",
    "* perform model diagnostics  \n",
    "* try to improve your model using data transformations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"data/ex2/ds_1.csv\")\n",
    "\n",
    "# Your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Fit your best model\n",
    "\n",
    "Now, we will wrap it all together. Let's try to fit our best model to the dataset about rental prices.  \n",
    "The data is available in `data/rent.csv`.\n",
    "\n",
    "Procedure:\n",
    "* Maka a model hypothesis\n",
    "* Fit a model\n",
    "* Run model diagnostics\n",
    "* Start again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f86fab27d6a8300e11571769dc7d43aaa28c36089531d106e36d5cf601e6df0f"
  },
  "kernelspec": {
   "display_name": "fantastic-features",
   "language": "python",
   "name": "fantastic-features"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
